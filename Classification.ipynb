{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c6c9fd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, gc, datetime\n",
    "\n",
    "import anndata\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import scipy.sparse as sp\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import src.load as load\n",
    "from src import Classifier\n",
    "from src.utils.general import load_reports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0470b2e7",
   "metadata": {},
   "source": [
    "<h3>Data</h3>\n",
    "\n",
    "Three datasets were used in this study.\n",
    "\n",
    "1. **Idiopathic Pulmonary Fibrosis (IPF)**: 96,301 cells and 4,443 highly variable genes. (https://www.science.org/doi/10.1126/sciadv.aba1983)\n",
    "2. **Mouse Cortex (MC)**: 3,005 cells and 20,006 genes. After filtering genes expressed in at least 10 cells, we are left with 16,484 genes. (https://www.science.org/doi/10.1126/science.aaa1934)\n",
    "3. **Human Cell Atlas (HCA)**: 84,363 cells and 2,968 highly variable genes. (https://genomebiology.biomedcentral.com/articles/10.1186/s13059-020-02210-0)\n",
    "\n",
    "These datasets can be downloaded as .h5ad files from https://drive.google.com/drive/folders/1fLt3QGI2XFIz-4ZjOwk9poFpmqq1Zcgt?usp=sharing.\n",
    "\n",
    "Since IPF and HCA are large, we only work with highly variable genes to reduce the runtime of the methods.\n",
    "The IPF h5ad file contains highly variable genes only, so we only need to set `high_var` to `True` for HCA.\n",
    "`filter_genes` will be set to True for both MC and HCA.\n",
    "\n",
    "Only for scGeneFit we also set `high_var=True` for MC. Without it, the algorithm will take too long to run across all random seeds and coverage factors.\n",
    "\n",
    "All data is total count normalized (`normalize = True`), log1p transformed (`log = True`) and scaled to unit variance and zero-mean (`scale = True`). Scanpy is performing the preprocessing in the backend.\n",
    "\n",
    "Finally, we remove all classes with less than 50 cells. This leads to 33 classes for IPF and 75 classes for HCA (tissue/cell type pairs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a9ee2ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Path('HCA') # choose from IPF, HCA, MC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3e43e49e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 unique cell types.\n",
      "adata.shape=(3005, 20006)\n",
      "Removed low count classes.\n",
      "adata.shape=(3005, 5034)\n",
      "7 celltype combinations.\n",
      "Range: 0.0 8.04193\n"
     ]
    }
   ],
   "source": [
    "root = 'data' / dataset\n",
    "os.makedirs(root, exist_ok=True)\n",
    "\n",
    "# Load data\n",
    "scale = True # set to False if running scGeneFit\n",
    "adata, key = load.dataset(\n",
    "    str(dataset),\n",
    "    normalize=True,\n",
    "    log=True,\n",
    "    scale=scale,\n",
    "    high_var=True if str(dataset) == 'HCA' else False,\n",
    "    #high_var=True, # Set manually to True for scGeneFit if MC\n",
    "    filter_genes=False if str(dataset) == 'IPF' else True,\n",
    ")\n",
    "adata = load.remove_low_count_ct(adata, key, 50)\n",
    "print(\"Range:\", adata.X.min(), adata.X.max())\n",
    "\n",
    "if not scale and sp.issparse(adata.X):\n",
    "    __mat = np.array(adata.X.todense())\n",
    "    __ad = anndata.AnnData(__mat)\n",
    "    __ad.obs = adata.obs\n",
    "    adata = __ad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af60b5d",
   "metadata": {},
   "source": [
    "<h2>Train and Test</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b589eef2",
   "metadata": {},
   "source": [
    "By default, we train a Logistic Regression model to predict the class label. The number of iterations is capped at 600 to speed up the experiments, although our experiments show that no significant improvement occurs if trained for more iterations.\n",
    "\n",
    "To use any other classifier, simply pass it to `base_classifier`. It must implement `fit_transform`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "13b29606",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test(\n",
    "        x_train, y_train,\n",
    "        x_test, y_test,\n",
    "        *,\n",
    "        feature_selector=None,\n",
    "        features=None,\n",
    "        json_path=None,\n",
    "        confusion_matrix_dir=None,\n",
    "        key=None,\n",
    "        base_classifier=None\n",
    "):\n",
    "    \"\"\"Train and test a classifier. Dump the classification results\n",
    "    and confusion matrices into a json_path and confusion_matrix_dir.\n",
    "    \"\"\"\n",
    "    # use logistic regression if None\n",
    "    if base_classifier is None:\n",
    "        base_classifier = LogisticRegression(\n",
    "            max_iter=600, verbose=0, n_jobs=-1)\n",
    "    # build a classifier wrapper with extra functionality\n",
    "    wrap_classifier = Classifier(\n",
    "        base_classifier,\n",
    "        feature_selector=feature_selector,\n",
    "        features=features,\n",
    "    )\n",
    "    # fit and save results\n",
    "    wrap_classifier.fit(x_train, y_train)\n",
    "    \n",
    "    if json_path is not None and confusion_matrix_dir is not None:\n",
    "        wrap_classifier.dump(\n",
    "            x_train, y_train,\n",
    "            x_test, y_test,\n",
    "            json_path=json_path,\n",
    "            confusion_matrix_dir=confusion_matrix_dir,\n",
    "            key=datetime.timestamp(datetime.now()) if key is None else key,\n",
    "            extras={'solution': (\n",
    "                feature_selector.get_support(indices=True)\n",
    "                if feature_selector is not None\n",
    "                else features\n",
    "            )})\n",
    "    else:\n",
    "        wrap_classifier.report(x_test, y_test)\n",
    "    print()\n",
    "    return wrap_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5a33b3",
   "metadata": {},
   "source": [
    "<h2>Run Classification</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532b6dd7",
   "metadata": {},
   "source": [
    "Note: You must first run the `FeatureSelection` notebook to create `report.json` files with selected features.\n",
    "\n",
    "Read the `report.json` files containing selected features from the corresponding directories, and run classification. We split the data in a train and test set of equal size in a stratified fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d8840f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "method_list = [\n",
    "    'GreedyCover',\n",
    "    'DT',\n",
    "    'TopDE',\n",
    "    'ReliefF',\n",
    "    'MI',\n",
    "    'mRMR',\n",
    "    'Fval',\n",
    "    'CrossEntropy',\n",
    "    'RankCorr',\n",
    "    'scGeneFit', # run this separately without scaled data for better results\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7fe2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for rs in range(42, 47):\n",
    "    # Split data into train/test\n",
    "    x_train, x_test = tts(\n",
    "        adata, random_state=rs,\n",
    "        stratify=adata.obs[key], train_size=0.5)\n",
    "    gc.collect()\n",
    "    \n",
    "    # Load reports for this random seed\n",
    "    root = 'data' / dataset / 'RS' / f'rs{rs}'\n",
    "    reports = load_reports(root, lreports=True, method_list=method_list)\n",
    "\n",
    "    for report, method in zip(reports, method_list):\n",
    "        coverage_list = list(report.keys())\n",
    "        for coverage in coverage_list:\n",
    "            solution = report[coverage]['solution']\n",
    "            basedir = root / method\n",
    "\n",
    "            _ = train_and_test(\n",
    "                x_train.X, x_train.obs[key],\n",
    "                x_test.X, x_test.obs[key],\n",
    "                features=solution,\n",
    "                json_path=basedir / 'logisticR.json',\n",
    "                confusion_matrix_dir=basedir / 'cm',\n",
    "                key=coverage,\n",
    "            )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
